# Working Scripts - Decimal Comparison Bug Research

This directory contains the **verified working scripts** that successfully demonstrate and validate the causal mechanisms behind the decimal comparison bug in Llama-3.1-8B-Instruct.

## üéØ Key Finding

**Layer 10 attention output is causally responsible for the decimal comparison bug.** This has been confirmed with 100% bidirectional causality - we can both fix and induce the bug by swapping attention outputs between correct and buggy prompt formats.

## üìÅ Scripts Overview

### 1. `bidirectional_patching.py` ‚≠ê MAIN RESULT
**Purpose**: Demonstrates complete bidirectional causality of Layer 10 attention output  
**Key Findings**:
- Forward Patching (Buggy format + Correct attention): **100% fixes the bug**
- Reverse Patching (Correct format + Buggy attention): **100% induces the bug**
- Proves Layer 10 attention output is definitively causal

**Usage**:
```bash
python bidirectional_patching.py
```

**Output**: 
- Console display showing both forward and reverse patching results
- Saves detailed results to `bidirectional_results_[timestamp].json`

---

### 2. `attention_control_experiment.py`
**Purpose**: Original working implementation that discovered Layer 10 causality  
**Key Findings**:
- First script to successfully patch attention outputs (not weights)
- Identified Layer 10 as the critical intervention point
- Shows other layers produce gibberish when patched

**Usage**:
```bash
python attention_control_experiment.py
```

**Output**: Tests multiple layers and shows Layer 10's unique success

---

### 3. `verify_llama_bug.py`
**Purpose**: Verifies the existence of the decimal comparison bug  
**Key Findings**:
- Confirms format-dependent behavior
- Simple format: 100% correct (9.8 > 9.11)
- Q&A/Chat formats: 100% buggy (9.11 > 9.8)

**Usage**:
```bash
python verify_llama_bug.py
```

**Output**: Shows bug occurrence across different prompt formats

---

### 4. `format_comparison.py`
**Purpose**: Tests multiple prompt formats to establish format-bug correlation  
**Key Findings**:
- 100% correlation between prompt format and bug occurrence
- Simple format always correct
- Q&A and Chat formats always trigger bug

**Usage**:
```bash
python format_comparison.py
```

**Output**: Comprehensive format testing results

---

### 5. `statistical_validation.py` ‚≠ê PUBLICATION-QUALITY VALIDATION
**Purpose**: Comprehensive statistical validation with n=1000 trials  
**Key Findings**:
- **100% success rate** for Layer 10 intervention (n=1000, p < 10‚Åª¬≥‚Å∞‚Å∞)
- **60% threshold** for minimum replacement needed
- **4/5 decimal pairs** show successful generalization
- **All 32 heads required** for intervention

**Usage**:
```bash
# Full validation (2-3 hours, n=1000)
python statistical_validation.py

# Quick test version available in script
```

**Output**: 
- `statistical_validation_results.json` - Complete numerical results
- `statistical_validation_visualization.png` - 6-panel results dashboard
- `FINAL_RESULTS.md` - Comprehensive report

## üî¨ Technical Details

### The Bug
The model incorrectly compares decimal numbers 9.8 and 9.11, saying 9.11 is bigger, but only in certain prompt formats.

### Prompt Formats
- **Correct Format**: `"Which is bigger: 9.8 or 9.11?\nAnswer:"`
- **Buggy Format**: `"Q: Which is bigger: 9.8 or 9.11?\nA:"`

### Why Layer 10?
- Layer 10's attention module processes the prompt structure
- The attention output (not weights) carries format-specific information
- Later layers (MLP and subsequent transformers) can process either format's attention output

### Key Distinction: Outputs vs Weights
- **Attention Weights** (softmax scores): NOT causal - patching these fails
- **Attention Outputs** (processed information): CAUSAL - patching these works

## üìä Success Metrics

| Experiment | Success Rate | Method |
|------------|--------------|--------|
| Forward Patch (Fix Bug) | 100% | Replace buggy attention with correct |
| Reverse Patch (Induce Bug) | 100% | Replace correct attention with buggy |
| Layer 10 Specificity | 100% | Only Layer 10 works cleanly |

## üöÄ Quick Start

To reproduce the main finding:
```bash
cd working_scripts
python bidirectional_patching.py
```

Expected output:
```
BIDIRECTIONAL ATTENTION OUTPUT PATCHING EXPERIMENT
==================================================

üìä TESTING BASELINES
- Correct format: "9.8 is bigger than 9.11" ‚úÖ
- Buggy format: "9.11 is bigger than 9.8" ‚ùå

üîß FORWARD PATCHING: Buggy format + Correct attention
Should FIX the bug if attention is causal
‚úÖ Result: "9.8 is bigger than 9.11"

üîÑ REVERSE PATCHING: Correct format + Buggy attention  
Should INDUCE the bug if attention is causal
‚ùå Result: "9.11 is bigger than 9.8"

üéâ BIDIRECTIONAL CAUSALITY CONFIRMED!
```

## üìà Implications

1. **Mechanistic Understanding**: We now know the bug originates from how Layer 10's attention processes different prompt formats
2. **Intervention Point**: Layer 10 attention output is the optimal intervention point
3. **Format Processing**: The attention mechanism encodes format-specific biases that affect numerical comparison
4. **Potential Fix**: Could potentially fix by modifying Layer 10's attention computation

## üìö Related Documentation

- See `/experimental_scripts/` for failed attempts and exploratory work
- See `BREAKTHROUGH_FINDINGS.md` in parent directory for theoretical background
- Original discovery documented in `/layer25/` directory

## ‚úÖ Validation

All scripts in this directory have been:
- Tested multiple times for reproducibility
- Verified to produce consistent results
- Confirmed to demonstrate the stated findings

---

*Last Updated: August 2024*  
*Research conducted on Llama-3.1-8B-Instruct model*